---
output:
  pdf_document: default
  html_document: default
title: "Environmental Influences and Rideshare Pricing"
author:
- Todor Dimitrov
- Cameron Sims
- Jose Octavio Hermida Rojas
- Hengchen Hao
---

``` {r pre-proc, echo=FALSE}
# Data Processing
data <- read.csv("rideshare_kaggle.csv")

# without non-temp data
dat <- data[, c("source","distance","price", "product_id", "temperature", "apparentTemperature", 
                "short_summary", "humidity", "windSpeed", "windGust", "visibility",
                "temperatureHigh", "temperatureLow", "pressure", "windBearing",
                "cloudCover", "uvIndex", "ozone", "moonPhase")]

# remove rows with NA
dat <- dat[complete.cases(dat[,"price"]),] # remove with NA price

lyft_premier <- dat[which(dat[,"product_id"] == "lyft_premier"),] #51235

# remove product_id now that we have split
lyft_premier$product_id <- NULL

lyft_premier <- lyft_premier[which(lyft_premier[,"source"] == "Back Bay"),]
lyft_premier$source <- NULL
lyft_premier <- lyft_premier[which(lyft_premier[,"distance"] <3),]
lyft_premier <- lyft_premier[which(lyft_premier[,"distance"] >2.5),]
lyft_premier$distance <- NULL
```

# Introduction:
  \hspace*{10mm}In a society that develops as amazingly as ours, a more convenient way of transportation has become crucial for us. The appearance of ride-share apps like Uber and Lyft seems to solve this problem very well; however, the complicated pricing rules behind this software has been bothering users. Users may notice that the similar trips with same fare type can end up with huge price differences, even they were only told that the apps are mostly charge by distance. In order to better help users understand the factors that affect the price of every trip, our group decided to study the impact of environmental factors on ride-share pricing.

  \hspace*{10mm}Our initial ideas and logic are very simple and straightforward. With worse weather conditions, people are less likely to walk or use public transportations, in this way, there is higher demand of ride-share, and the price will be higher. While using the app, we can easily notice that there is always a pop-up surge multiplier notice when the weather condition is bad, or during late night or rush hours.

  \hspace*{10mm}The public literatures that can be referred are very limited. We only found one paper from Oklahoma State University that analyzed the relationship between weather type and number of ride-shares in Boston. In the paper, the key finding is that transitions in weather play a crucial role in the number of rides called (Gutha, Mamillapalli 2016). Besides this report, there have not been a great deal of scholarly reports. We are limited to interviews of rideshare executives explaining how weather dictates and influences surge pricing (Delaney 2016). With the lack of existing research about this topic, our group decided to test few models, then discuss whether those models are fit for our topic. We also wanted to discuss if some models can help us make perditions of future trip fares with particular weather conditions.


# Methods:
  \hspace*{10mm}To prove our hypothesis about how weather affects rideshare pricing, we explore and analyze the “Uber and Lyft Dataset Boston, MA” (BM 2019) data set. This data set contained the information of 693071 trips from a variety of different products of Uber and Lyft, with all trips taking place in Boston during November through December of 2018. The data set contains basic information about each trip, such like price, distance, surge multiplier, and product type. In addition, there are each trip has a variety of environmental factors—temperature, humidity, wind speed, etc.

  \hspace*{10mm}Overall, there are 57 different variables for each trip, with 40 of the relating to weather, and the other 17 relating to details about the trip itself (distance, price, product, etc.). A lot of the predictors, however, seemed to carry less relevant weather information, like `apparentTemperatureMinTime ` and `sunriseTime`. So we opted to focus on the following set of variables to predict price changes: temperature, apparentTemperature, short_summary, humidity, windSpeed, windGust, visibility, temperatureHigh, temperatureLow, pressure, windBearing, cloudCover, uvIndex, ozone, and moonPhase.

  \hspace*{10mm}In addition to this, it was necessary that we considered other factors that could potentially affect the price of a ride or trips in which the information was incomplete. To account for this, we first eliminated any rides that did not contain all the established variables. Then we further limited our data set my focusing on Lyft Premier rides with a starting point of Black Bay that had a distance between 2.5 and 3 miles. This ensures that the variability in prices is not due to the change in product (different products have different quality and therefore different prices), distance, or geographical area in Boston. We have accounted for several factors and now end up with a data set of 433 data points which we can analyze in order to find the relationship between weather conditions and price.

# Results:
We will now go into a discussion of the various models considered and their outcomes.

## Linear Models:
  \hspace*{10mm}Our first attempt at modeling the relationship between various weather predictors and rideshare price was a linear model. We first fit using all the available predictors. 

```{r linear-all, echo=FALSE, warning=FALSE}
library(leaps)
library(knitr)
cur_data <- lyft_premier
cur_data$short_summary <- NULL

cur_data.lm <- lm(price~., data=cur_data)

X = model.matrix(cur_data.lm)

kable(summary(cur_data.lm)$coefficients,
      caption = "Table of coefficients for linear model with all predictors",
      digits = 1)
```

  \hspace*{10mm}This model does not explain the variablity we see in price. It has an r-squared value of `r sprintf("%.3f",summary(cur_data.lm)$r.squared)`. In other words, less than 5% of the variablity we see in price is explained by our model.

  \hspace*{10mm}As we can see from Table (1), the only significant coefficient is moonPhase. However, this is likely due to the complexity of the model, as fitting price with only moonPhase lead to a non-significant coefficient. Furthermore, running stepwise selection to find the best subset of predictors also supports this conclusion.

``` {r lm-all-regfit, echo=FALSE,fig.height=3, fig.cap="\\label{fig:lm-all-regfit}Plots of Mallow's Cp and RSS for various model sizes"}
fit <- regsubsets(X, cur_data$price, intercept=FALSE, nvmax=14, method="forward")

par(mfrow=c(1,2))
x <- seq(1,length(fit$rss),1)
plot(x, fit$rss,type = "l", xlab = "size", ylab = "RSS", main="size vs RSS")

plot(x, summary(fit)$Cp, type="l", xlab="size", ylab="Cp", main="size vs Cp")
```

  \hspace*{10mm}From Figure \ref{fig:lm-all-regfit}, we see that RSS decreases with increased model complexity, as expected. However, we do not see the expected quadratic shape for Mallow's Cp index. Rather we see a positive linear increase, meaning that a model of size 1 (only intercept) is optimal. This means that we are best off simply taking an average of our response (price).

  \hspace*{10mm}Moving on, our next step was to limit our predictors. We decided to choose the following 5: windSpeed, ApparentTemperature, uvIndex, pressure, and humidity. Here is our reasoning for selecting these. We would expect that with high wind speeds, indviduals are less likely to walk and will therefore order more lyfts. Similarly, a high uvIndex is unhealthy and so we would expect a linear relationship. A high humidity would make walks less bearable and therefore we would expect a higher demand of rideshares. Low pressure is associated with higher likelihood of rain and under worse conditions we would expect more rides to be hailed. Lastly, at extreme apparent temperatures, we would expect individuals to be less likely to walk - a quadratic relationship between temperature and demand.

``` {r interaction-plots, echo=FALSE,fig.height=4,fig.width=10, fig.cap="\\label{fig:interaction-plots}Scatterplots of every pair of predictors"}
pairs(lyft_premier[,c("windSpeed","apparentTemperature","uvIndex","pressure","humidity")], cex.labels=1.25, bg='orange', cex=.3)
```

  \hspace*{10mm}From Figure \ref{fig:interaction-plots} we can see a positive correlation between WindSpeed and apparentTemperature, as well as between humidity and apparentTemperature. We also see a negative correlation between pressure and WindSpeed. To account for this within our model, we added interaction terms between these predictors. Further, to account for assumption of apparentTemperature being quadratic, we added ploynomial terms for the temperature predictor.

```{r linear-small, echo=FALSE}
library(leaps)
library(knitr)
cur_data <- lyft_premier
cur_data$short_summary <- NULL

cur_data.lm <- lm(price~windSpeed+poly(apparentTemperature,2)+uvIndex+pressure+humidity
                  +windSpeed:apparentTemperature
                  +humidity:apparentTemperature
                  +pressure:windSpeed, data=cur_data)

X = model.matrix(cur_data.lm)

kable(summary(cur_data.lm)$coefficients,
      caption = "Table of coefficients for linear model with limited predictors",
      digits = 1)
```

  \hspace*{10mm}We see that the increased complexity from using all the predictors did a better job of explaining the variability in price as using only a subset of the variables with interaction terms actually led to a decrease of r-squared to `r sprintf("%.3f",summary(cur_data.lm)$r.squared)`.

  \hspace*{10mm}As we can see from Table (2), there are no significant coefficients under the significance level $\alpha = 0.05$. Running stepwise selection to find the best subset of predictors gives a similar outcome to using all the predictors.

``` {r lm-small-regfit, echo=FALSE,fig.height=3, fig.cap="\\label{fig:lm-small-regfit}Plots of Mallow's Cp and RSS for various model sizes"}
fit <- regsubsets(X, cur_data$price, intercept=FALSE, nvmax=14, method="forward")

par(mfrow=c(1,2))
x <- seq(1,length(fit$rss),1)
plot(x, fit$rss,type = "l", xlab = "size", ylab = "RSS", main="size vs RSS")

plot(x, summary(fit)$Cp, type="l", xlab="size", ylab="Cp", main="size vs Cp")
```

  \hspace*{10mm}From Figure \ref{fig:lm-all-regfit}, we see that a model of size 1 (only intercept) is optimal. This means that we are best off simply taking an average of our response (price). 

  \hspace*{10mm}We see that through simple linear and non-linear models, we were unable to account for the variability in price for rideshares.

## Threshold Analysis:

  \hspace*{10mm}Our next analysis was on the precentage of rides we see above a certian threshold. As we can see from Figure \ref{fig:emp-boot-thres} there is a steep dropoff at $25 so our intuition is this the percentage of rides above this threshold can be an indicator of demand.

``` {r emp-boot-thres,echo=FALSE,fig.height=3, fig.cap="\\label{fig:emp-boot-thres}Plot of price distribution (left) Plot of boostrap distribution (Right)"}
par(mfrow=c(1,2))

# just straight up fitting exponential using mle for lambda
lambda.hat <- 1/mean(lyft_premier[,"price"]-15)

x <-0:35
hist(lyft_premier[,"price"]-15, col="tan", probability=T, xlab="Price - 15", main="Distribution of Price")
lines(x,dexp(x,rate=lambda.hat), col=2)
legend("topright", legend="mle exp", col=2, lwd=2)

cur_data <- lyft_premier

thres <- 25

B <- 10000
above.orig <- length(which(cur_data$price>thres))/nrow(cur_data)

# holder of boostrap sample percentage
above.BT <- rep(NA, B)

for (i in 1:B) {
  samp <- sample(nrow(cur_data), nrow(cur_data), replace=T)
  BT <- cur_data[samp,]
  above.BT[i] <- length(which(BT$price>thres))/nrow(BT)
}

hist(above.BT, col="tan", breaks=10,main="Hist of % above threshold", xlab="%")
abline(v=above.orig, col="red", lwd=3)
legend("topright", legend="orig %", col="red", lwd=3, cex = 0.7)

basic <- quantile(above.BT, c(0.05,0.95)) # quantile method
```

  \hspace*{10mm}Figure \ref{fig:emp-boot-thres} (Left) is a distribution of the rideshare prices we see with an exponential distribution with the maximum likelihood estimate for $\lambda$. As we can see the fit is not quite exponential.

  \hspace*{10mm}Next, we performed a bootstrap analysis of the percentage of rides we see above our threshold ($25) to estimate our true population value. From, Figure \ref{fig:emp-boot-thres} (Right) we can see that bootstrap samples had percentages approximately normally distributed around our sample value. The 90% confidence interval for the population value is: [`r sprintf("%.3f",basic[1])`, `r sprintf("%.3f",basic[2])`]. Hence, we are 90% confident that the true population percentage is between `r sprintf("%.3f",basic[1])` and `r sprintf("%.3f",basic[2])`. 

## Logistic Models

  \hspace*{10mm}
Following our threshold analysis, we performed logistic regression on the relationship between moonPhase and price, as well as, pressure and price. We appended a column to our data labeled **above** which took a value of 1 if the price for that specific observation was above the threshold ($25).

``` {r log,echo=FALSE,fig.height=4, fig.cap="\\label{fig:log}MoonPhase logistic (left) Pressure logistic (Right)"}
cur_data <- lyft_premier
thres <- 25              # desired threshold for price

# add indicator variable
cur_data$above <- as.numeric(cur_data$price > thres)

# fit basic logistic to data
fit_logistic_m = glm(above ~ moonPhase, data=cur_data, 
                   family = "binomial")

aic.m <- summary(fit_logistic_m)$aic

# 90% CI
xs = data.frame(moonPhase = seq(from=0.09,to=0.93, by=.001))
pred = predict(fit_logistic_m, type="response", newdata=xs, se=T)

p.m = pred$fit
p_U.m = pred$fit+qnorm(0.95)*pred$se.fit
p_L.m = pred$fit-qnorm(0.95)*pred$se.fit

par(mfrow=c(1,2))

plot(xs$moonPhase, p.m, type="l", lwd=6, col="blue", xlab="moonPhase", ylim=c(0,1), 
     ylab="Probability of having price above 25")
points(cur_data$moonPhase, cur_data$above, col=2)
lines(xs$moonPhase, p_U.m, col="dodgerblue", lwd=3,
      lty=2)
lines(xs$moonPhase, p_L.m, col="dodgerblue", lwd=3,
      lty=2)

# pressure fit
fit_logistic_p = glm(above ~ pressure, data=cur_data, 
                   family = "binomial")

aic.p <- summary(fit_logistic_p)$aic

# 90% CI
xs = data.frame(pressure = seq(from=988,to=1035, by=.5))
pred = predict(fit_logistic_p, type="response", newdata=xs, se=T)

p.p = pred$fit
p_U.p = pred$fit+qnorm(0.95)*pred$se.fit
p_L.p = pred$fit-qnorm(0.95)*pred$se.fit

plot(xs$pressure, p.p, type="l", lwd=6, col="blue", xlab="pressure",ylim=c(0,1),
     ylab="Probability of having price above 25")
points(cur_data$pressure, cur_data$above, col=2)
lines(xs$pressure, p_U.p, col="dodgerblue", lwd=3,
      lty=2)
lines(xs$pressure, p_L.p, col="dodgerblue", lwd=3,
      lty=2)
```

  \hspace*{10mm}Figure \ref{fig:log} shows the logistic fits for both moonPhase and pressure plotted with the  as well as their respective 90% confidence intervals. As we can see from these plots, these fits are not very successful. We see a higher probability of seeing increased variability for a higher moonPhase (closer to full moon) and for lower pressure. The AIC scores for moonPhase is `r sprintf("%.1f",aic.m)` and pressure is `r sprintf("%.1f",aic.p)`. Both are extremely high meaning we likely cannot draw any inferences from the fit. We see a similar trend regardless of what predictor we use.

## Permutation test

  \hspace*{10mm}Since we were having trouble finding significant predictors for price, we decided to test if the mean prices between the different weather types were significant. To do this, we performed a permutation test between the mean prices of each weather type, which included a lot of combinations. In doing so, we were able to determine which differences between weather types were significant.

```{r permutations,echo=FALSE}
perm_data <- data.frame(xx=character(), yy=character(), perm_result=double(),
                        stringsAsFactors = F)

# Function to perform permutation test between means
perm_test <- function(xx, yy){
  # Calculate means and difference
  xx_mean <- mean(xx)
  yy_mean <- mean(yy)
  diff_mean <- abs(xx_mean - yy_mean)
  
  # Calculate lengths and combined length
  n_x <- length(xx)
  n_y <- length(yy)
  n <- n_x + n_y
  
  # Combine data
  data_pull <- c(xx, yy)
  
  # Perform permutation test
  n_per <- 1000
  diff_mean_per <- rep(NA, n_per)
  set.seed(160) # For reproducible results
  for (i_per in 1:n_per){
    w_per <- sample(n, n, replace=T)
    data_per <- data_pull[w_per]
    datax_new <- data_per[1:n_x]
    datay_new <- data_per[(n_x+1):n]
    diff_new <- abs(mean(datax_new)-mean(datay_new))
    diff_mean_per[i_per] <- diff_new
  }
  
  # Return p-value
  return((length(which(diff_mean_per>diff_mean))+1)/n_per)
}

# Permutation Tests for means between prices depending on weather
for (i in 1:8){
  
  # Get and set name for xx
  xx <- lyft_premier[lyft_premier$short_summary==unique(lyft_premier$short_summary)[i],]
  
  for (j in i:8) {
    
    # Get and set name for yy
    yy <- lyft_premier[lyft_premier$short_summary==unique(lyft_premier$short_summary)[j+1],]
    
    # Get and set permutation result
    perm_result <- perm_test(xx$price, yy$price)
    
    # Add data to perm_data
    new_df <- data.frame(xx=as.character(xx$short_summary[1]),
                         yy=as.character(yy$short_summary[1]),
                         perm_result=perm_result)
    perm_data <- rbind(perm_data, new_df)
  }
}

# Reorder perm_data based on p-vals
perm_data <- perm_data[order(perm_data$perm_result),]

colnames(perm_data) <- c("Weather type 1", "Weather type 2", "p-value")

kable(perm_data[1:10,],
      caption = "Table top 10 significant permutation tests",
      digits = 1)
```

  \hspace*{10mm}The permutation tests managed to reveal significant differences between mean prices for different weathers, such as "Overcast" and "Light Rain". From Table (3) we can see the top ten combinations in terms of statistical significance.  Overall, there were 6 differences with p-values less than 10%, 2 of which were below 5%. Examining Table (3), it is interesting to see what weathers had significantly different mean prices and how significant these differences were. For example, "Overcast" and "Light Rain" are somewhat similar weathers, yet they had the most significant difference. 

\newpage
## Conclusion. 

  \hspace*{10mm}While a large portion of our tests and models were unsuccessful, we did see intriguing results for our permutation test. Some of these differences we would intuitively expect. It is understandable that "Clear" and "Light Rain" would have a difference in mean prices as we would expect demand to be larger for the worse weather conditon. However, interestingly enough, we did not see every combination we would expect to have significant results. For example, we did not find evidence of a difference between the means of "Clear" and "Rain". Perhaps this can be accounted for the limited number of observations we have in our dataset. These permutation tests gave us an idea on how to proceed with future analyses, and it would be interesting to do continued analysis using the divided datasets.

  \hspace*{10mm}However, most of our attempts to fit a relationship on the data were rather unsuccessful. This most likely can be attributed to our distribution of prices. Figure \ref{fig:price} is a plot of price and apparentTemperature. As we can see, price is not continious. Rather, we see horizontal bands develop. This makes us believe that Lyft's pricing algorithm is a constant multiplied by a surge parameter which is altered based on Lyft's estimation of the demand (likely intervals) and for this reason we see these horizontal lines develop. However, this could also possibly be attributed to how the data was collected. Rideshare companies do not make their pricing information public. This is to prevent price manipulation by the drivers. Because of this, the data was likely mined and perhaps the prices were estimated or rounded. Another possible explanantion is that this dataset had rides limited to about 4 miles. Perhaps if we had access to a dataset with rides of larger distances, we would see more varaition in the pricing and our models would have more success. 

  \hspace*{10mm}While unsuccessful, these horizontal bands gave us the intuiton to do perform the threshold and logistic analysis as we see increased variability for prices above $25. For further analysis, we would like to do a similar analysis on rides of longer length. We believe that our methods would likely see more success. In addition, our paper was focused on Lyft rides. Similar analysis to what was seen in this paper should be performed on an Uber dataset to see if there are significant differences between the two companies.
  
``` {r price,echo=FALSE,fig.height=4, fig.width=6, fig.cap="\\label{fig:price}Scatterplot of apparentTemperature and price"}
plot(lyft_premier$apparentTemperature, lyft_premier$price, xlab="apparentTemperature", ylab="price", main="temp vs price")
abline(h=mean(lyft_premier$price), col=2)
legend("top", legend="mean price", col=2,lwd=1, cex=.7)
```

\newpage
## References:
BM. (2019, December). Uber and Lyft Dataset Boston, MA, Version 2. Retrieved 6 May, 2020 from

  \hspace*{10mm}https://www.kaggle.com/brllrb/uber-and-lyft-dataset-boston-ma/activity.
  
Delaney, Dave. “Surge Pricing: Why a Rainy Day Ride Will Cost You More.” The Tennessean, 

  \hspace*{10mm}The Tennessean, 29 May 2016,

  \hspace*{10mm}www.tennessean.com/story/money/2016/05/29/surge-pricing-why-rainy-day-ride-cost-you-more/84922758/

Gutha, Snigdha and Anusha, Mamillapalli. “Analyzing the Effect of Weather on Uber Ridership.”

  \hspace*{10mm}MWSUG , 2016, support.sas.com/resources/papers/proceedings17/1260-2017.pdf.