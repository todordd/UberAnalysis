---
output:
  pdf_document: default
  html_document: default
---
Stat/QSci 403         Final Project   
=======================================================
Names go here
Cameron Sims
-------------------------------------------------------

Here is how I am preprocessing the data:

Some things to notice:

- all datapoints are from november-december 2018 in Boston (month param probably kinda useless)
- product_id and name refer to if it is a luxury cab, a plus ... 
- distance is between 0-4 mainly (some extremes to max of about 8)
- almost all datapoints have surge multiplier on??
- short_summary has 9 levels (might be good enough -> ignore long_summary)
- precip intensity seems to have a very small range and too little cases where it isnt 0
- precip Probability could be used in some sort of logistic prediction?
- mainly avoided timestamps
- icon seems to be very similar to the summaries
- not really sure what dewPoint is but ill include it for now
- uvIndex seems like a nice catagorical variable for us to use -> only 3 levels 
- visibility vs visibility.1? no clue how they differ
- I think the moon phase variable is sweet
- not sure how temperatureMin and temperatureMax differ from temperatureHigh and temperatureLow
- there are also apparent temperature max and lows but i left those out for now.

Here are the temp data
  - Price -> real number
  - temperature -> real number
  - short_summary -> catagorical (9 lvls)
  - humidity -> real number
  - windSpeed -> real number
  - windGust -> real number
  - visibility -> real number
  - temperatureHigh -> real number
  - temperatureLow -> real number
  - pressure -> real number
  - windBearing -> integer
  - cloudCover -> real number
  - uvIndex -> integer
  - ozone -> real number
  - moonPhase -> real number
  - product_id -> factor (13 levels but only 6 usable)

The only data with NA was price with (55095). I removed these rows completely.


``` {r pre-proc}
# Data Processing
data <- read.csv("rideshare_kaggle.csv")
dat <- data[, c("source","product_id", "destination", "cab_type","price", "distance",
               "temperature", "apparentTemperature", "short_summary",
               "humidity", "windSpeed", "windGust", "visibility", 
               "temperatureHigh", "temperatureLow", "pressure", "windBearing",
               "cloudCover", "uvIndex", "ozone", "moonPhase")]

# without non-temp data
dat <- data[, c("price", "product_id", "temperature", "apparentTemperature", "short_summary",
               "humidity", "windSpeed", "windGust", "visibility", 
               "temperatureHigh", "temperatureLow", "pressure", "windBearing",
               "cloudCover", "uvIndex", "ozone", "moonPhase")]

# remove rows with NA
dat <- dat[complete.cases(dat[,"price"]),] # remove with NA price

lyft <- dat[which(dat[,"product_id"] == "lyft"),]                 #51235
lyft_line <- dat[which(dat[,"product_id"] == "lyft_line"),]       #51233
lyft_lux <- dat[which(dat[,"product_id"] == "lyft_lux"),]         #51235
lyft_luxsuv <- dat[which(dat[,"product_id"] == "lyft_luxsuv"),]   #51235
lyft_plus <- dat[which(dat[,"product_id"] == "lyft_plus"),]       #51235
lyft_premier <- dat[which(dat[,"product_id"] == "lyft_premier"),] #51235

# remove product_id now that we have split
lyft$product_id <- NULL
lyft_line$product_id <- NULL
lyft_lux$product_id <- NULL
lyft_luxsuv$product_id <- NULL
lyft_plus$product_id <- NULL
lyft_premier$product_id <- NULL
```

Here are some things we are planning on doing:
- lm on all the variables (some of these are catagorical so we will need to create dummy variables)
- forward/backward stepwise selction on the variables
- KDE using cross val to estimate bandwidth
- maybe consider doing some sort of regression tree (easy to interpret -> see which vars are most important)
  - if we go down this road we could do boosting trees which use boostrap
  
```{r stepwise}
library(leaps)
# Create test/train sets
train=sample(c(TRUE,FALSE), nrow(dat),rep=TRUE)
test=(!train)

# Apply regsubsets to training set
regfit.best=regsubsets(price~.,data=dat[train,],nvmax=19)

# Make model matrix from test data
test.mat=model.matrix(price~.,data=dat[test,])

# Compute test MSE for models of different sizes
val.errors=rep(NA,19)
for(i in 1:19){
  coefi=coef(regfit.best,id=i)
  pred=test.mat[,names(coefi)]%*%coefi
  val.errors[i]=mean((dat$price[test]-pred)^2)
}

# Plot of data
plot(seq(1,19), val.errors, main="Test MSE vs. Number of Predictors",
     xlab="Number of Predictors", ylab="Test MSE", type='l')
points(which.min(val.errors), min(val.errors), col='red')


# Best model with optimal number of variables
regfit.best=regsubsets(price~.,data=dat,nvmax=19)
coef(regfit.best,which.min(val.errors))
```