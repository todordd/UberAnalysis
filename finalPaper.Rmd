---
output:
  pdf_document: default
  html_document: default
---
Stat/QSci 403         Final Project   
=======================================================
Names go here
Cameron Sims
-------------------------------------------------------

Here is how I am preprocessing the data:

Some things to notice:

- all datapoints are from november-december 2018 in Boston (month param probably kinda useless)
- product_id and name refer to if it is a luxury cab, a plus ... 
- distance is between 0-4 mainly (some extremes to max of about 8)
- almost all datapoints have surge multiplier on??
- short_summary has 9 levels (might be good enough -> ignore long_summary)
- precip intensity seems to have a very small range and too little cases where it isnt 0
- precip Probability could be used in some sort of logistic prediction?
- mainly avoided timestamps
- icon seems to be very similar to the summaries
- not really sure what dewPoint is but ill include it for now
- uvIndex seems like a nice catagorical variable for us to use -> only 3 levels 
- visibility vs visibility.1? no clue how they differ
- I think the moon phase variable is sweet
- not sure how temperatureMin and temperatureMax differ from temperatureHigh and temperatureLow
- there are also apparent temperature max and lows but i left those out for now.

Here are the temp data
  - Price -> real number
  - temperature -> real number
  - short_summary -> catagorical (9 lvls)
  - humidity -> real number
  - windSpeed -> real number
  - windGust -> real number
  - visibility -> real number
  - temperatureHigh -> real number
  - temperatureLow -> real number
  - pressure -> real number
  - windBearing -> integer
  - cloudCover -> real number
  - uvIndex -> integer
  - ozone -> real number
  - moonPhase -> real number
  - product_id -> factor (13 levels but only 6 usable)

The only data with NA was price with (55095). I removed these rows completely.


``` {r pre-proc}
# Data Processing
data <- read.csv("rideshare_kaggle.csv")
dat <- data[, c("source","product_id", "destination", "cab_type","price", "distance",
               "temperature", "apparentTemperature", "short_summary",
               "humidity", "windSpeed", "windGust", "visibility", 
               "temperatureHigh", "temperatureLow", "pressure", "windBearing",
               "cloudCover", "uvIndex", "ozone", "moonPhase")]

# without non-temp data
dat <- data[, c("source","distance","price", "product_id", "temperature", "apparentTemperature",                    "short_summary", "humidity", "windSpeed", "windGust", "visibility", 
               "temperatureHigh", "temperatureLow", "pressure", "windBearing",
               "cloudCover", "uvIndex", "ozone", "moonPhase")]

# remove rows with NA
dat <- dat[complete.cases(dat[,"price"]),] # remove with NA price

lyft <- dat[which(dat[,"product_id"] == "lyft"),]                 #51235
lyft_line <- dat[which(dat[,"product_id"] == "lyft_line"),]       #51233
lyft_lux <- dat[which(dat[,"product_id"] == "lyft_lux"),]         #51235
lyft_luxsuv <- dat[which(dat[,"product_id"] == "lyft_luxsuv"),]   #51235
lyft_plus <- dat[which(dat[,"product_id"] == "lyft_plus"),]       #51235
lyft_premier <- dat[which(dat[,"product_id"] == "lyft_premier"),] #51235

# remove product_id now that we have split
lyft$product_id <- NULL
lyft_line$product_id <- NULL
lyft_lux$product_id <- NULL
lyft_luxsuv$product_id <- NULL
lyft_plus$product_id <- NULL
lyft_premier$product_id <- NULL

##### STILL NEED TO PURGE REMAINING IF GONNA USE
lyft_premier <- lyft_premier[which(lyft_premier[,"source"] == "Back Bay"),]
lyft_premier$source <- NULL
lyft_premier <- lyft_premier[which(lyft_premier[,"distance"] <3),]
lyft_premier <- lyft_premier[which(lyft_premier[,"distance"] >2.5),]
lyft_premier$distance <- NULL
```

Here are some things we are planning on doing:
- lm on all the variables (some of these are catagorical so we will need to create dummy variables)
- forward/backward stepwise selction on the variables
- KDE using cross val to estimate bandwidth
- maybe consider doing some sort of regression tree (easy to interpret -> see which vars are most important)
  - if we go down this road we could do boosting trees which use boostrap
  
```{r stepwise}
library(leaps)
# Create test/train sets
train=sample(c(TRUE,FALSE), nrow(dat),rep=TRUE)
test=(!train)

# Apply regsubsets to training set
regfit.best=regsubsets(price~.,data=dat[train,],nvmax=19)

# Make model matrix from test data
test.mat=model.matrix(price~.,data=dat[test,])

# Compute test MSE for models of different sizes
val.errors=rep(NA,19)
for(i in 1:19){
  coefi=coef(regfit.best,id=i)
  pred=test.mat[,names(coefi)]%*%coefi
  val.errors[i]=mean((dat$price[test]-pred)^2)
}

# Plot of data
plot(seq(1,19), val.errors, main="Test MSE vs. Number of Predictors",
     xlab="Number of Predictors", ylab="Test MSE", type='l')
points(which.min(val.errors), min(val.errors), col='red')


# Best model with optimal number of variables
regfit.best=regsubsets(price~.,data=dat,nvmax=19)
coef(regfit.best,which.min(val.errors))
```

```{r testing-models}

cur_data <- lyft_premier
cur_data$short_summary <- NULL
# pairs(lyft[,2:ncol(lyft)], cex.labels=3, pch=3, bg='orange', cex=2)

#cur_data.lm <- lm(price~., data=cur_data)
cur_data.lm <- lm(price~windSpeed+windGust+ozone+pressure + windSpeed:windGust, cur_data)
#ozone:windSpeed:windGust+pressure:windSpeed:windGust+windSpeed:windGust + windSpeed:ozone+ windGust:ozone+ windSpeed:pressure+ windGust:pressure
#cur_data.lm <- lm(price~ozone+pressure+ozone:pressure, cur_data)
summary(cur_data.lm)$coefficients[2,4]
summary(cur_data.lm)$coefficients
X = model.matrix(cur_data.lm)[,-1]

fit <- regsubsets(X, cur_data$price, intercept=FALSE, nvmax=14, method="forward")

x <- seq(1,length(fit$rss),1)
plot(x, fit$rss,type = "l", xlab = "k", ylab = "RSS", main="k vs RSS")

plot(x, fit$rss/(100*(1-(3*x+1)/100)^2), type="l", xlab="k", ylab="GCV", main="k vs modified GCV score")
```

which Variables do we need interaction terms:
``` {r interaction-terms}

pairs(lyft_premier[,c("windSpeed","windGust","ozone","cloudCover","uvIndex")], cex.labels=3, pch=23, bg='orange', cex=.1)

pairs(lyft_premier[,c("windSpeed","windGust","ozone","humidity","pressure")], cex.labels=1, pch=23, bg='orange', cex=.1)

pairs(lyft_premier[,c("windSpeed","windGust","ozone","temperature","pressure")], cex.labels=1, pch=23, bg='orange', cex=.1)

# need for sure
plot(lyft_premier[,"windSpeed"], lyft_premier[,"windGust"])

#maybe
plot(lyft_premier[,"windSpeed"], lyft_premier[,"ozone"])
```


``` {r smoothing}
pred <- lyft_premier[,"temperature"]
output <- lyft_premier[,"price"]

samp <- sample(length(pred),4*length(pred)/5)
x.train <- pred[samp]
y.train <- output[samp]
x.test <- pred[-samp]
y.test <- output[-samp]

ksmooth.train <- function(x.train, y.train, kernel = c("box", "normal"), bandwidth = 0.5, CV = FALSE) {
  X <- x.train
  Y <- y.train
  # sort the data
  ind <- order(X)
  Y <- Y[ind]
  X <- X[ind]
  
  ret <- list(x.train = c(), yhat.train = c())
  if (kernel == "box"){
    for (i in 1:length(X)) {
      num <- 0
      sum <- 0
      cur <- X[i]
      # find every x within bandwidth
      for (other in 1:length(X)) {
        if (CV && (other == i)) {
          next
        }
        if (abs(X[other] - cur) <= bandwidth) {
          num <- num + 1
          sum <- sum + Y[other]
        }
      }
      ret$x.train <- c(ret$x.train, cur)
      ret$yhat.train <- c(ret$yhat.train, sum/num)
    }
  } else {
    # normal kernel smoothing
    # find sd of normal where quartiles are +/-0.25% bandwidth
    sd <- (bandwidth/4)/qnorm(.75,0,1)
    for (i in 1:length(X)) {
      num <- 0
      den <- 0
      cur <- X[i]
      for (other in 1:length(X)){
        if (CV && (other == i)) {
          next
        }
        temp <- dnorm(cur-X[other], 0, sd)
        num <- num + Y[other]*temp
        den <- den + temp
      }
      ret$x.train <- c(ret$x.train, cur)
      ret$yhat.train <- c(ret$yhat.train, num/den)
    }
  }
  return(ret)
}

ksmooth.predict <- function(ksmooth.train.out, x.query) {
  X <- ksmooth.train.out$x.train
  Y <- ksmooth.train.out$yhat.train
  # sort and order
  ind <- order(X)
  Y <- Y[ind]
  X <- X[ind]
  
  y.predicted <- c()
  for (i in 1:length(x.query)) {
    cur <- x.query[i]
    if (cur < X[1]) {
      # below smallest X data
      y.predicted <- c(y.predicted, Y[1])
      next
    } else if (cur > X[length(X)]) {
      # above largest X data
      y.predicted <- c(y.predicted, Y[length(Y)])
      next
    } else if (!is.na(match(cur, X))) {
      # X contains cur return its yhat
      y.predicted <- c(y.predicted, Y[match(cur,X)])
      next
    }
    # find two closest points where cur is in between
    ind.aft <- 1
    while (X[ind.aft] < cur) {
      ind.aft <- ind.aft + 1
    }
    # use point slope to find interpolated y
    m <- (Y[ind.aft] - Y[ind.aft-1])/(X[ind.aft] - X[ind.aft-1])
    y.predicted <- c(y.predicted, Y[ind.aft] + m*(cur-X[ind.aft]))
  }
  return(y.predicted)
}

plot(x.train, y.train, xlab="windGust", ylab="price", main="windGust vs price TRAINING data")
Kreg = ksmooth.train(x=x.train,y=y.train, kernel = "normal",bandwidth = 2)
lines(Kreg$x.train, Kreg$yhat.train, lwd=4, col="orange")

plot(x.test, y.test, xlab="windGust", ylab="price", main="windGust vs price TEST data")
pred <- ksmooth.predict(Kreg, x.test)
X <- x.test
Y <- pred
# sort the data
ind <- order(X)
Y <- Y[ind]
X <- X[ind]
lines(X, Y,lwd=4, col = "orange")
```

``` {r parametric-normal}
# just straight up fitting exponential using mle for mean and sd
lambda.hat <- 1/mean(lyft_premier[,"price"]-15)

x <-0:35
hist(lyft_premier[,"price"]-15, col="tan", probability=T)
lines(x,dexp(x,rate=lambda.hat), col=2)
```



``` {r turbo-attempt}
# using only temperature
truncated.power.design.matrix <- function(x) {
  n <- length(x)
  design.m <- matrix(NA, nrow=n, ncol = n)
  for (i in 1:(n-1)) {
    cur <- x[i] # so current basis is (x-cur)_+
    for (j in 1:n) {
      if (x[j] <= cur) {
        design.m[j,i] <-0
      } else {
        design.m[j,i] <- x[j] - cur
      }
    }
  }
  design.m[,n] <- rep(1,n)
  return(design.m)
}

X <- truncated.power.design.matrix(cur_data$temperature[1:100])
y <- cur_data$price[1:100]

regsubsets.fitted.values <- function(X, regsubsets.out, nterm) {
  return(regsubsets(X, regsubsets.out, intercept=FALSE, nvmax=nterm, method="forward"))
}

# best subset for up to k basis functions k = [1,100]
k <-10
regfit <- regsubsets.fitted.values(X,y,k)

x <- seq(1,k,1)
plot(NULL, xlim= c(0,k), ylim=c(0,50), xlab = "k", ylab = "RSS", main="k vs RSS")
lines(x, regfit$rss)

x <- seq(1,32,1)
plot(NULL, xlim= c(0,32), ylim=c(0,50), xlab="k", ylab="GCV", main="k vs modified GCV score")
lines(x, regfit$rss[1:32]/(100*(1-(3*x+1)/100)^2))
```

```{r permutations}
perm_data <- data.frame(xx=character(), yy=character(), perm_result=double(),
                        stringsAsFactors = F)

# Function to perform permutation test between means
perm_test <- function(xx, yy){
  # Calculate means and difference
  xx_mean <- mean(xx)
  yy_mean <- mean(yy)
  diff_mean <- abs(xx_mean - yy_mean)
  
  # Calculate lengths and combined length
  n_x <- length(xx)
  n_y <- length(yy)
  n <- n_x + n_y
  
  # Combine data
  data_pull <- c(xx, yy)
  
  # Perform permutation test
  n_per <- 1000
  diff_mean_per <- rep(NA, n_per)
  for (i_per in 1:n_per){
    w_per <- sample(n, n, replace=T)
    data_per <- data_pull[w_per]
    datax_new <- data_per[1:n_x]
    datay_new <- data_per[(n_x+1):n]
    diff_new <- abs(mean(datax_new)-mean(datay_new))
    diff_mean_per[i_per] <- diff_new
  }
  
  # Return p-value
  return((length(which(diff_mean_per>diff_mean))+1)/n_per)
}

# Permutation Tests for means between prices depending on weather
for (i in 1:8){
  
  # Get and set name for xx
  xx <- dat[dat$short_summary==unique(dat$short_summary)[i],]
  
  for (j in i:8) {
    
    # Get and set name for yy
    yy <- dat[dat$short_summary==unique(dat$short_summary)[j+1],]
    
    # Get and set permutation result
    perm_result <- perm_test(xx$price, yy$price)
    
    # Add data to perm_data
    new_df <- data.frame(xx=as.character(xx$short_summary[1]),
                         yy=as.character(yy$short_summary[1]),
                         perm_result=perm_result)
    perm_data <- rbind(perm_data, new_df)
  }
}
```

``` {r emp-boot-thres}
cur_data <- lyft_premier

thres <- 25

B <- 10000
above.orig <- length(which(cur_data$price>thres))/nrow(cur_data)

# holder of boostrap sample median
above.BT <- rep(NA, B)

for (i in 1:B) {
  samp <- sample(nrow(cur_data), nrow(cur_data), replace=T)
  BT <- cur_data[samp,]
  above.BT[i] <- length(which(BT$price>thres))/nrow(BT)
}

hist(above.BT, col="tan", breaks=10,main="Histogram of % above threshold")
abline(v=above.orig, col="red", lwd=3)
legend("topright", legend="original sample %", col="red", lwd=3)

v <- var(above.BT) # bootstrap variance of sample median

mse <- mean((above.BT - above.orig)^2) # boostrap estimate of mse from sample median

basic <- quantile(above.BT, c(0.05,0.95)) # quantile method

v
mse
basic

```